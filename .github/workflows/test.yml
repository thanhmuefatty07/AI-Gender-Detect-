# ============================================
# Automated Testing Pipeline
# ============================================

name: Test

on:
  schedule:
    # Run daily at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:

jobs:
  integration-test:
    runs-on: ubuntu-latest

    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_PASSWORD: postgres
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

      redis:
        image: redis:7
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -e .[dev]

    - name: Run integration tests
      run: |
        pytest tests/integration/ -v --tb=short
      env:
        DATABASE_URL: postgresql://postgres:postgres@localhost:5432/test_db
        REDIS_URL: redis://localhost:6379

    - name: Run API tests
      run: |
        # Start API server in background
        python inference/api/main.py &
        sleep 10

        # Run API tests
        pytest tests/e2e/ -v --tb=short

        # Stop server
        pkill -f "python inference/api/main.py"

  performance-test:
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt

    - name: Run performance benchmarks
      run: |
        python -m pytest tests/ -k "performance" -v --benchmark-only

    - name: Store benchmark results
      uses: benchmark-action/github-action-benchmark@v1
      with:
        name: Python Benchmark
        tool: 'pytest'
        output-file-path: output.json
        github-token: ${{ secrets.GITHUB_TOKEN }}
        auto-push: true

  load-test:
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install locust

    - name: Run load tests
      run: |
        # Start API server
        python inference/api/main.py &
        sleep 10

        # Run load tests
        locust -f tests/load/locustfile.py --headless -u 100 -r 10 --run-time 1m --csv=load_test_results

        # Stop server
        pkill -f "python inference/api/main.py"

    - name: Upload load test results
      uses: actions/upload-artifact@v3
      with:
        name: load-test-results
        path: load_test_results_*.csv

  data-validation:
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt

    - name: Validate sample datasets
      run: |
        python scripts/validate_datasets.py --datasets datasets/academic/utkface/

    - name: Check data quality metrics
      run: |
        python scripts/check_data_quality.py --input datasets/prepared/train/

  notify:
    runs-on: ubuntu-latest
    needs: [integration-test, performance-test, load-test, data-validation]
    if: always()

    steps:
    - name: Send notification
      run: |
        if [ "${{ needs.integration-test.result }}" == "success" ] && \
           [ "${{ needs.performance-test.result }}" == "success" ] && \
           [ "${{ needs.load-test.result }}" == "success" ] && \
           [ "${{ needs.data-validation.result }}" == "success" ]; then
          echo "✅ All tests passed!"
        else
          echo "❌ Some tests failed"
        fi
